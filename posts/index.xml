<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>梯度下降</title>
    <link>https://ljwh.github.io/posts/</link>
    <description>Recent content on 梯度下降</description>
    <image>
      <title>梯度下降</title>
      <url>https://ljwh.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://ljwh.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 11 Sep 2023 20:25:05 +0800</lastBuildDate><atom:link href="https://ljwh.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GD</title>
      <link>https://ljwh.github.io/posts/gd/</link>
      <pubDate>Mon, 11 Sep 2023 20:25:05 +0800</pubDate>
      
      <guid>https://ljwh.github.io/posts/gd/</guid>
      <description>梯度 梯度是一个数学概念，梯度函数表示的是多元可微函数的向量域函数，在某一点的梯度是该多元可微函数在该点上的偏导数为分量的向量。
我们用导数衡量一元函数的变化情况，类似的，梯度是导数在多元函数中的推广，就像一元函数的导数表示这个函数图形的切线的斜率，如果多元函数在该点上的梯度不是零向量，则它的方向是这个函数在该点上最大增长的方向、而它的量是在这个方向上的增长率。
上面是维基百科对梯度的解释，更直观的理解是：在多元函数的任意一点，对任意方向都有一个变化率，将变化率最大的那个方向向量叫做梯度，大小（向量的模）就是沿这个方向的变化率。
梯度下降 梯度下降(Gradient Descent)由法国数学家奥古斯丁·路易斯·柯西在1847年提出。
梯度下降法最直观的理解就是按照函数变化最大的方向（梯度方向）进行迭代搜索，直到进入一个局部最小值。
想象一个简单的例子，我们在一个小型的沙漠中，我们所处的位置可以用经度、纬度以及海拔高度来表示，经度和纬度为我们的自变量，也就是我们可以通过移动来改变经纬度。海拔高度是我们的损失函数，假设目标是得到最小的损失函数，那我们需要通过移动经纬度来达到一个海拔高度最低的点。
Ok, 如果这个沙漠不大，类似一个碗的形状，那么我们靠视力就可以判断出海拔最低的点（解析解）。
但是如果现在是夜晚，伸手不见五指，我们只能看到方圆1米的情况，那么我们怎么找到这个海拔最低的点？
我们先环顾四周，找到下坡的方向（梯度），移动一米后继续观察四周，找到下坡的方向（梯度），继续移动&amp;hellip;
一直到我们发现四周都是上坡的时候，我们就到了海拔最低的点了（数值解）。
解析解，又称为闭式解（英语：Analytic expression），是可以用解析表达式来表达的解。 在数学上，如果一个方程或者方程组存在的某些解，是由有限次常见运算的组合给出的形式，则称该方程存在解析解。二次方程的根就是一个解析解的典型例子。在低年级数学的教学当中，解析解也被称为公式解。 当解析解不存在时，比如五次以及更高次的代数方程，则该方程只能用数值分析的方法求解近似值。大多数偏微分方程，尤其是非线性偏微分方程，都只有数值解。
解析解和数值解 - 维基百科
当我们不能一眼看出沙漠的最低点时（没有解析解），可以通过慢慢移动或其他的方式逼近最低点（数值解）。用不同的方式代表着不同的优化算法，而梯度下降的方式显然是最直观也较容易理解的。毕竟你要去沙漠的最低点，总是会想到沿着下坡的方向。
动量(Momentum) 引入了动量的梯度下降法，增加了训练速度，降低了训练时（特别是随机梯度下降）的震荡现象。
先来看看动量，动量是一个物理学概念，我们都知道动量p的公式: $$\vec{p} = m\vec{v}$$, m是物体的质量，v是物体的速度。动量法的梯度下降考虑了历史的梯度信息，为了不再有更多的公式出现，我们来描述一下动量法梯度下降的更新过程，每次的梯度更新需要考虑上一次的梯度，在上一次梯度的基础上通过叠加本次梯度的更新来进行调整，一般来说上一次梯度的权重$$\beta$$为0.9（经验值）。
$$ v_t = \beta v_{t-1} + \eta\nabla_\theta J(\theta) \ \theta = \theta - v_t $$ 动量法为什么会增加训练速度？来看看训练动画：
动量法是一个非常直观和容易理解的，很符合我们生活中的直觉：将小球从起始点放下，那么它的运动轨迹大体上和动量法的训练轨迹类似。
为什么能更快的训练呢？动量法叠加了历史的梯度信息，在一直下降的方向上，更新的会越来越快。
动量所累积的速度还能有助于冲出一些局部最小点：
在使用梯度下降法（特别是随机梯度下降）进行迭代时，如果有其中一个维度方向的变化非常大，体现在3d图中就是非常深的峡谷，那么梯度就会沿着这个方向来回震荡，如下图所示：
加入动量的方法是容易直观理解的，震荡的原因是因为每次迭代的方向几乎改变了180度，如果我们限制每次迭代方向的变化幅度，那么就可以很大程度上减少震荡的幅度。一个形象的比喻是开车的时候每次打方向都要考虑上一次的方向，从而不至于方向变化幅度的太大而导致震荡
2012年技惊四座的AlexNet就是用动量法的梯度下降进行训练。</description>
    </item>
    
  </channel>
</rss>
