{"meta":{"title":"LJWH","subtitle":null,"description":null,"author":"liujian","url":"http://jeremyliu.org"},"pages":[{"title":"About Me","date":"2018-05-13T16:00:00.000Z","updated":"2020-05-14T03:34:45.548Z","comments":true,"path":"about/index.html","permalink":"http://jeremyliu.org/about/index.html","excerpt":"","text":"你好，如果有任何问题，可以通过电子邮箱liu.c2n@gmail.com来联系我。"}],"posts":[{"title":"RNN 源码解析（tensorflow）","slug":"tensorflow-rnn","date":"2018-04-16T16:00:00.000Z","updated":"2018-05-14T16:05:41.063Z","comments":true,"path":"2018/04/17/tensorflow-rnn/","link":"","permalink":"http://jeremyliu.org/2018/04/17/tensorflow-rnn/","excerpt":"","text":"本文将基于tensorflow r1.5的源码，从组成RNN的细胞开始，到如何构建一个RNN，学习这整个的流程。 rnn_cell的接口声明 tensorflow在tensorflow/python/ops/rnn_cell.py中声明了一个RNNCell基类和几个基本的RNN cell类,这些类的实现在tensorflow/python/ops/rnn_cell_impl.py中。接下来，我们具体来看看这些类的实现。 RNNCell类RNNCell类继承了base_layer.Layer类，是RNN cell的一个抽象类的表示。 __call__()方法会调用父类base_layer.Layer类的__call__()方法,父类base_layer.Layer类的__call__()方法会调用call方法，所以所有继承了RNNCell的类，只需要重写自身的call方法，就可以实现__call__()的调用。 __call__()的输入主要是inputs, state, 输出是outputs, new_sate,通过调用这个方法，实现序列采样的前进。 下面介绍几种常见的RNN cell的具体实现。 BasicRNNCell类 BasicRNNCell类是RNNCell类最基本的实现。 构造方法中传入参数主要是num_units(神经元个数)。 该类的call()方法的具体实现如下： 12345678def call(self, inputs, state): \"\"\"Most basic RNN: output = new_state = act(W * input + U * state + B).\"\"\" gate_inputs = math_ops.matmul( array_ops.concat([inputs, state], 1), self._kernel) gate_inputs = nn_ops.bias_add(gate_inputs, self._bias) output = self._activation(gate_inputs) return output, output 通过方法的注释可以直观的看到，每次调用call()方法时，也就是序列采样时，output = new_state = act(W input + U state + B).在计算时，将input和state串联起来一起计算，节省时间。 这里要注意的是，inputs的shape是(batch_size, seq_length), output和state是同样的, shape为(batch_size, num_units), 参数W和U串联在一起的self._kernel的shape是(seq_length + num_units, num_units). BasicLSTMCell类 BasicLSTMCell类是LSTM cell的基本实现。 构造方法中传入参数主要是num_units(神经元个数)。 该类的call()方法的具体实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445def call(self, inputs, state): \"\"\"Long short-term memory cell (LSTM). Args: inputs: `2-D` tensor with shape `[batch_size, input_size]`. state: An `LSTMStateTuple` of state tensors, each shaped `[batch_size, self.state_size]`, if `state_is_tuple` has been set to `True`. Otherwise, a `Tensor` shaped `[batch_size, 2 * self.state_size]`. Returns: A pair containing the new hidden state, and the new state (either a `LSTMStateTuple` or a concatenated state, depending on `state_is_tuple`). \"\"\" sigmoid = math_ops.sigmoid one = constant_op.constant(1, dtype=dtypes.int32) # Parameters of gates are concatenated into one multiply for efficiency. if self._state_is_tuple: c, h = state else: c, h = array_ops.split(value=state, num_or_size_splits=2, axis=one) gate_inputs = math_ops.matmul( array_ops.concat([inputs, h], 1), self._kernel) gate_inputs = nn_ops.bias_add(gate_inputs, self._bias) # i = input_gate, j = new_input, f = forget_gate, o = output_gate i, j, f, o = array_ops.split( value=gate_inputs, num_or_size_splits=4, axis=one) forget_bias_tensor = constant_op.constant(self._forget_bias, dtype=f.dtype) # Note that using `add` and `multiply` instead of `+` and `*` gives a # performance improvement. So using those at the cost of readability. add = math_ops.add multiply = math_ops.multiply new_c = add(multiply(c, sigmoid(add(f, forget_bias_tensor))), multiply(sigmoid(i), self._activation(j))) new_h = multiply(self._activation(new_c), sigmoid(o)) if self._state_is_tuple: new_state = LSTMStateTuple(new_c, new_h) else: new_state = array_ops.concat([new_c, new_h], 1) return new_h, new_state 可以看到，函数输入中的state是一个tuple, 这是因为LSTM网络比一般的RNN网络多出了一个cell_state. 接着计算gate_inputs, 也就是4个门分别的输入，这里是把4个门的输入的参数串联到一个参数矩阵，一次计算得到一个输入矩阵，包含了4个门的分别的不同的输入，然后把矩阵切分开来，得到了i, j, f, o这4个门的输入。 根据公式计算c和h，c = forget_gate * c + input_gate * activation(input),h = activation(c) * output_gate. 由于BasicLSTMCell不包含投影层，所以output就是h, new_state是(c, h). LSTMCell类 与BasicLSTMCell相比，LSTMCell类提供了可选的peep-hole链接、cell clipping以及投影层。 该类的call()方法的具体实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273def call(self, inputs, state): \"\"\"Run one step of LSTM. Args: inputs: input Tensor, 2D, `[batch, num_units]. state: if `state_is_tuple` is False, this must be a state Tensor, `2-D, [batch, state_size]`. If `state_is_tuple` is True, this must be a tuple of state Tensors, both `2-D`, with column sizes `c_state` and `m_state`. Returns: A tuple containing: - A `2-D, [batch, output_dim]`, Tensor representing the output of the LSTM after reading `inputs` when previous state was `state`. Here output_dim is: num_proj if num_proj was set, num_units otherwise. - Tensor(s) representing the new state of LSTM after reading `inputs` when the previous state was `state`. Same type and shape(s) as `state`. Raises: ValueError: If input size cannot be inferred from inputs via static shape inference. \"\"\" num_proj = self._num_units if self._num_proj is None else self._num_proj sigmoid = math_ops.sigmoid if self._state_is_tuple: (c_prev, m_prev) = state else: c_prev = array_ops.slice(state, [0, 0], [-1, self._num_units]) m_prev = array_ops.slice(state, [0, self._num_units], [-1, num_proj]) input_size = inputs.get_shape().with_rank(2)[1] if input_size.value is None: raise ValueError(\"Could not infer input size from inputs.get_shape()[-1]\") # i = input_gate, j = new_input, f = forget_gate, o = output_gate lstm_matrix = math_ops.matmul( array_ops.concat([inputs, m_prev], 1), self._kernel) lstm_matrix = nn_ops.bias_add(lstm_matrix, self._bias) i, j, f, o = array_ops.split( value=lstm_matrix, num_or_size_splits=4, axis=1) # Diagonal connections if self._use_peepholes: c = (sigmoid(f + self._forget_bias + self._w_f_diag * c_prev) * c_prev + sigmoid(i + self._w_i_diag * c_prev) * self._activation(j)) else: c = (sigmoid(f + self._forget_bias) * c_prev + sigmoid(i) * self._activation(j)) if self._cell_clip is not None: # pylint: disable=invalid-unary-operand-type c = clip_ops.clip_by_value(c, -self._cell_clip, self._cell_clip) # pylint: enable=invalid-unary-operand-type if self._use_peepholes: m = sigmoid(o + self._w_o_diag * c) * self._activation(c) else: m = sigmoid(o) * self._activation(c) if self._num_proj is not None: m = math_ops.matmul(m, self._proj_kernel) if self._proj_clip is not None: # pylint: disable=invalid-unary-operand-type m = clip_ops.clip_by_value(m, -self._proj_clip, self._proj_clip) # pylint: enable=invalid-unary-operand-type new_state = (LSTMStateTuple(c, m) if self._state_is_tuple else array_ops.concat([c, m], 1)) return m, new_state 和BasicLSTMCell类似，这里同样是获取state，其中包含了c_prev和m_prev, 分别表示了上一步中的cell和hidden的状态。 接着计算4个门的输入，和BasicLSTMCell类似，不同点是，在这个地方，为4个门的输入张量加上了一个bias。 下面是计算c和h，如果配置了使用peepholes connections，那么会在计算forget, input和output这三个门的输入上，分别串联前一个时刻的c. 如果配置了num_proj，那么会将m（hidden state）投影到配置的num_proj的维度上。 构建RNN的几种方法 现在我们已经对RNN的cell有所了解了，那么该如何创建一个完整的RNN呢？ 我们已经知道，对RNN的cell的call()函数的循环调用，可以实现序列的采样，那么最直观的想法就是我们构建一个循环，不停的调用call()函数, 这样就得到了一个RNN。基本思想如下： 123456state = cell.zero_state(...)outputs = []for input_ in inputs: output, state = cell(input_, state) outputs.append(output)return (outputs, state) 在tensorflow中，已经有一些帮助方法，可以帮我们快速的构建一个RNN，比如： tf.nn.static_rnn() tf.nn.dynamic_rnn() tf.nn.bidirectional_dynamic_rnn() tf.nn.raw_rnn() 下面我们来一一学习这些帮助方法。 tf.nn.static_rnn()tf.nn.static_rnn()的主要输入参数有5个：cell是RNN指定的细胞； inputs是输入，类型必须是collections.Sequence(除了string)， shape为(B, T, D)， 如果只有一个序列，那么可以为1维，即(D)； initial_state是给定的RNN的其实状态，如不指定，则以RNNCell.zero_state(…)来初始化，且必须要指定dtype； sequence_length是序列的长度，如果指定了序列的长度，那么会启动动态计算，即在一个batch内，超过sequence_length的部分将不予以计算。 函数内部，首先取出inputs中第一个输入，然后检查所有的batch的size是否一致，最后得到batch_size。 tf.nn.dynamic_rnn()tf.nn.dynamic_rnn()与tf.nn.static_rnn()有些区别，首先在图的定义上，dynamic_rnn()构造的是一个可以循环执行的图，序列的长度表示为循环的次数；而static_rnn()构造的是一个RNN的展开图，所以对于不同的batch，dynamic_rnn()可以允许不同的batch内有不同的序列长度，而statci_rnn()由于展开图的长度就是序列的长度，所以在不同的batch内必须要有相同的长度。后续会写一篇文章来详细的说明tf.nn.static_rnn()和tf.nn.dynamic_rnn()的异同及性能。 一般推荐使用dynamic_rnn()取代static_rnn(). tf.nn.bidirectional_dynamic_rnn()bidirectional_dynamic_rnn()是dynamic_rnn()的双向版本。 为什么要有双向的RNN呢？本人认为这是RNN的结构造成的，RNN与一般的全连接神经网络的一个主要的不同是，RNN可以在不同的输入中共享参数，但是RNN是序列结构，所以只能是后面的输入共享前面的输入的信息，前面的输入不能得到后面的信息，所以的在实际的应用中（如machine translation)，序列的采样仅仅从左到右或者从右到左都是会丢失一些信息，这时候就需要双向的RNN，tensorflow提供了这个帮助方法tf.nn.bidirectional_dynamic_rnn()。 序列前向输入RNN的结果为(output_fw, output_state_fw)，序列后向输入RNN的结果为(output_fw, output_state_fw), 总的输出为((output_fw, output_state_fw), (output_fw, output_state_fw)). tf.nn.raw_rnn()tf.nn.raw_rnn()是tf.nn.dynamic_rnn()更底层的函数。dynamic_rnn()有一些限制，tf.nn.raw_rnn()可以提供更底层的控制，以便于实现一些特殊的需求，如seq2seq模型的解码等等。 函数的输入中，比较重要的是loop_fn, loop_fn也是一个函数，输入为(time, cell_output, cell_state, loop_state), 输出为(finished, next_input, next_cell_state, emit_output, next_loop_state). loop_fn会在循环每一次结束之后调用，使我们更多的控制循环的进行，如更改循环的输出，下一个循环的输入，细胞的状态。 结束至此，你已经大体的了解了tensorflow中RNN的具体实现。","categories":[],"tags":[{"name":"RNN","slug":"RNN","permalink":"http://jeremyliu.org/tags/RNN/"},{"name":"Tensorflow","slug":"Tensorflow","permalink":"http://jeremyliu.org/tags/Tensorflow/"},{"name":"源码解析","slug":"源码解析","permalink":"http://jeremyliu.org/tags/源码解析/"}]},{"title":"深度学习与中文分词","slug":"chinese-segment","date":"2017-12-16T16:00:00.000Z","updated":"2020-05-14T03:35:39.799Z","comments":true,"path":"2017/12/17/chinese-segment/","link":"","permalink":"http://jeremyliu.org/2017/12/17/chinese-segment/","excerpt":"","text":"大家好，本来我是打算经常更新这里的文章的，但是事情好像总是很多，最近受了一位朋友的影响，更觉得要握紧时间了。下面进入正题： 什么是中文分词？根据宗成庆老师所著的《统计自然语言处理》中的描述：由于词是语言的能够独立运用的最小语言单位，所以计算机在对自然语言进行处理之前，都要进行分词的处理。 而在很多西方语言中，词与词之间是有空格之类的显著的区别标志，这使得分词的工作较为轻松。 而在汉语的很多语言中，词与词之间没有明显的界限，自动分词技术称为了很多自然语言处理的基本环节，同时这可以认为是一个尚未完全解决的问题。 为什么是深度学习？随着深度学习的发展，越来越多的项目开始拥抱深度学习，中文分词这个课题也不例外。为了让我们对这个演变的过程有个清晰的认识，所以姑且在此把中文分词分为3个阶段。 第一个阶段是完全基于词典的分词（机械分词），这个阶段的分词效果很差，对于歧义不能很好的识别，需要有一份预置的词典，词典的粒度不容易控制，但是，分词速度很快，在此期间诞生了很多优秀的基于词典的匹配算法（如前向最大匹配算法）。 第二个阶段是引入机器学习中的基于N-Gram语言模型训练的时间序列模型（如HMM）。 值得一提的是，Nianwen Xue老师在Chinese Word Segmentation as Character Tagging提出的基于字标注的模型，被后续至今的几乎所有的模型采用。著名的开源项目jieba中，是在机械分词的基础上，引入了HMM模型对于连续的单字进行处理以提升效果。之所以说是引入，是由于以下的几个原因导致了这个阶段的时间序列模型多数会和中文分词结合使用，才能达到更好的效果。 由于N-Gram模型的限制，训练语料中远处的信息被未能得到利用，对于一些语义歧义不能很好的判断。时间序列中的参数矩阵规模较大，难以训练，统一分词标准的大规模训练语料难以获取，人工标注代价很高。第三个阶段是近期兴起的仍在前行的尚未大规模应用的基于RNN的分词模型。因为上述的机器学习中的训练语料中的远处信息未能得到利用，关于这一问题，可以参考邱锡鹏教授在Long Short-Term Memory Neural Networks for Chinese Word Segmentation提出的经典的例子： 冬 天 (winter)， 能 (can) 穿 (wear) 多 少 (amount) 穿 (wear) 多少 (amount)；夏天 (summer)，能 (can) 穿 (wear) 多 (more) 少 (little) 穿 (wear) 多 (more) 少 (little)。 RNN在理论上可以解决这个问题，可是由于梯度消失的问题，一般的RNN充分训练较为困难，而其变种LSTM在此方面工作的较为出色。关于LSTM的工作原理，可以参考这篇著名的文章Understanding LSTM Networks。 字嵌入+双向LSTM+CRF的模型在这里，我们介绍一种目前较为流行的基于字标注的字向量+双向LSTM+CRF的模型，模型如下所示： 图片来自于邱锡鹏教授近期的论文Adversarial Multi-Criteria Learning for Chinese Word Segmentation。 可以看到，针对待分词的语句，将其每个字都转换为字向量，转换的方式可以使用Word2vec，也可以使用一些其他的向量化模型，而且这个向量化模型可以是通用的，不必针对训练语料单独训练一个向量化模型。并且LSTM模型的输入一般是固定长度的，我们需要定义一个固定的长度，对小于/大于这个长度的输入，进行填充/截断，这个长度一般为120以内，因为现在大部分模型，都难以处理超过120的输入。 在得到固定长度的字向量之后，将其作为BiLSTM的输入，LSTM开始训练，得到每个词在字标注上概率分布，再根据CRF模型，得到最终的字标注结果。 在参考了一些开源项目的情况下，我已经基于Tensoflow使用Python接口实现了这个模型，并在公司内部得到了使用，训练语料包括一些公开的如SIGHAN上的语料，还包括一些购买的语料。由于公司的信息安全的规定，目前还未能公开所有的代码及语料。 目前存在的问题事情不总是一帆风顺的，深度学习提升了分词的效果，但是也有一些问题，根据理论和一些实际的经验，在此我认为的列出： 需要更大规模的训练语料，难以获取。 距离较远的词的信息更加难以学习，训练时间更长。 就分词而言，更远距离的词，对分词结果影响没有想象的大。 模型的分词性能较为一般。","categories":[],"tags":[]},{"title":"Gradle:下一代Maven？","slug":"intro-gradle","date":"2017-07-15T16:00:00.000Z","updated":"2018-05-13T23:01:25.715Z","comments":true,"path":"2017/07/16/intro-gradle/","link":"","permalink":"http://jeremyliu.org/2017/07/16/intro-gradle/","excerpt":"","text":"Hi，你们好，这是我在这里写的第一篇文章，希望自己能坚持更新哦。刚好最近在看Gradle，那么我想，就从ta开始吧。 Gradle是什么？Gradle是一个基于Apache Ant和Apache Maven概念的项目自动化建构工具。它使用一种基于Groovy的特定领域语言来声明项目设置，而不是传统的XML。 我为什么会使用Gradle第一次使用Gradle，是当时我在学校读书因为一个项目而需要学习Android时的事情。有一些Android项目是用Gradle来管理的。那时候使用的ide是Android Studio。 虽然才过了一年多，但是因为我毕业后再也没有参与过Android的项目，也没有再使用过Gradle来管理项目，所以关于Gradle的很多东西，也已经记不得了。 由于最近工作中的一个项目，我又接触到了Gradle，现在我把自己对Gradle的一些理解写在这里，希望能对一些人有所帮助。:P Gradle是怎么工作的关于安装Gradle，我不准备在此写下，去官网看下安装的说明，应该不是很难的事情。 Gradle通过Grovvy来编写配置，这和其他基于XML的工具来说，是有很大的灵活性的，因为在很多时候你要通过XML来实现一些逻辑操作是相当麻烦的。而使用一门编程语言来实现这些逻辑操作就没有那么麻烦了。XML更适合机器阅读，当你有大量依赖且需要排除依赖的时候，你的XML简直没法看了。 当然如果只是简单的替换掉XML，我们可能不需要有Gradle，因为SBT也做到了 ：P Gradle取消了依赖仓库的概念，特别是在管理项目的多个模块的时候会非常友好。 Gradle与Maven的比较一种较为主流的观点是，在JVM平台，Ant是第一代的项目管理工具，Maven是第二代，而Gradle是下一代Maven。 为什么Gradle能有如此高的评价呢，对比Maven，我想可能由于以下几个方面的优点： 不使用XML来申明依赖，更加简洁。 可以使用Groovy来编写task，比起Maven的Plugin更加灵活和方便。 项目的多模块之间的管理更加方便。 使用task，可以隔离不同的操作。 根据Gradle官网上的介绍Gradle与Maven的性能对比，Gradle的性能比Maven高出很多，这在一些大型项目中，是不得不考虑的因素之一。 但是目前使用Maven的人还是很多，从Maven转换到Gradle，从这些优点来看，考虑到这不是很低的学习成本，是不是动力不足呢？很多人总是习惯现有的东西，需要别的人来推动他。","categories":[],"tags":[]}]}